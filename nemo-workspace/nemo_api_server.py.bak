#!/usr/bin/env python3
"""
NeMo FastAPI Server for AI Assistant
Runs inside NeMo Docker container to provide inference API
"""
import os
import logging
import asyncio
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

import torch
import nemo
import nemo.collections.nlp as nemo_nlp

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global model storage
_loaded_model = None
_model_info = {}

class ChatMessage(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_length: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40

class ChatResponse(BaseModel):
    response: str
    model_info: Dict[str, Any]
    generation_time: float

class ModelInfo(BaseModel):
    model_name: str
    model_type: str
    parameters: Optional[str] = None
    gpu_memory_used: Optional[str] = None
    cuda_available: bool
    device: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan - load model on startup"""
    logger.info("Starting NeMo API Server...")
    await load_default_model()
    yield
    logger.info("Shutting down NeMo API Server...")

app = FastAPI(
    title="NeMo Inference API",
    description="NeMo-powered inference server for AI Assistant",
    version="1.0.0",
    lifespan=lifespan
)

async def load_default_model():
    """Load default NeMo model for inference"""
    global _loaded_model, _model_info
    
    try:
        logger.info("Loading NeMo model...")
        
        # Try to load a pre-trained GPT model
        # You can replace this with your preferred NeMo model
        model_name = os.getenv("NEMO_MODEL_NAME", "gpt2")
        
        # Load MegatronGPT-20B model with proper trainer setup
        logger.info("Setting up MegatronGPT-20B model...")
        try:
            from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel
            
            # Import the correct Lightning trainer version for NeMo
            try:
                from lightning.pytorch import Trainer
                logger.info("Using lightning.pytorch.Trainer")
            except ImportError:
                try:
                    from pytorch_lightning import Trainer
                    logger.info("Using pytorch_lightning.Trainer")
                except ImportError:
                    logger.error("No compatible Lightning trainer found")
                    raise
            
            # Initialize distributed backend for single GPU inference
            os.environ['MASTER_ADDR'] = 'localhost'
            os.environ['MASTER_PORT'] = '12355'
            os.environ['RANK'] = '0'
            os.environ['WORLD_SIZE'] = '1'
            
            # Initialize process group and model parallel state for MegatronGPT
            try:
                if not torch.distributed.is_initialized():
                    torch.distributed.init_process_group(
                        backend='nccl' if torch.cuda.is_available() else 'gloo',
                        rank=0,
                        world_size=1
                    )
                    logger.info("Initialized distributed process group for MegatronGPT")
                
                # Initialize model parallel state for single GPU inference
                import megatron.core.parallel_state as parallel_state
                if not parallel_state.model_parallel_is_initialized():
                    parallel_state.initialize_model_parallel(
                        tensor_model_parallel_size=1,
                        pipeline_model_parallel_size=1,
                        virtual_pipeline_model_parallel_size=None,
                        context_parallel_size=1
                    )
                    logger.info("Initialized model parallel state for single GPU")
                    
            except Exception as e:
                logger.warning(f"Could not initialize parallel states: {e}")
            
            # Create minimal trainer for MegatronGPT model loading
            logger.info("Creating minimal trainer for MegatronGPT...")
            trainer = Trainer(
                devices=1 if torch.cuda.is_available() else "auto",
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision="16-mixed" if torch.cuda.is_available() else 32,
                enable_checkpointing=False,
                logger=False,
                enable_model_summary=False,
                max_epochs=1
            )
            
            # Try to load MegatronGPT-20B model first
            model_name = os.getenv("MEGATRON_MODEL", "nvidia/megatron-gpt-20b")
            logger.info(f"Attempting to load MegatronGPT model: {model_name}")
            
            try:
                # Check available models first
                available_models = MegatronGPTModel.list_available_models()
                logger.info(f"Available MegatronGPT models: {[m.pretrained_model_name for m in available_models]}")
                
                # Try to load the largest available MegatronGPT model
                _loaded_model = MegatronGPTModel.from_pretrained("megatron_gpt_345m", trainer=trainer)
                logger.info("Successfully loaded MegatronGPT-345M with trainer")
                
                # Model loaded successfully with parallel state initialization
                # The set_inference_config method requires a different approach
                logger.info("MegatronGPT parallel states are properly initialized for single-GPU inference")
                
                # Note: For MegatronGPT-20B, we'll need to manually download from NGC
                logger.info("Note: MegatronGPT-20B requires manual download from NVIDIA NGC")
                
            except Exception as e:
                logger.warning(f"Failed to load MegatronGPT with trainer: {e}")
                raise
            
            # Set to eval mode for inference
            _loaded_model.eval()
            logger.info("MegatronGPT model ready for inference")
                
        except Exception as e:
            logger.warning(f"Failed to load MegatronGPT model: {e}")
            # Fallback to larger HuggingFace model for better responses
            try:
                from transformers import pipeline
                logger.info("Loading larger HuggingFace model...")
                
                # Try larger models first - aim for production quality
                model_options = [
                    "microsoft/DialoGPT-large",           # 762M params, conversational
                    "gpt2-xl",                           # 1.5B params, largest GPT-2  
                    "microsoft/DialoGPT-medium",         # 354M params, conversation
                    "gpt2-large",                        # 774M params
                    "gpt2-medium",                       # 345M params
                    "gpt2"                               # 124M params (last resort)
                ]
                
                for model_name in model_options:
                    try:
                        _loaded_model = pipeline(
                            "text-generation", 
                            model=model_name, 
                            device=0 if torch.cuda.is_available() else -1,
                            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                        )
                        logger.info(f"Successfully loaded {model_name}")
                        break
                    except Exception as model_error:
                        logger.warning(f"Failed to load {model_name}: {model_error}")
                        continue
                else:
                    raise Exception("Failed to load any fallback model")
                    
            except Exception as e2:
                logger.error(f"Failed to load any model: {e2}")
                _loaded_model = None
        
        # Get model info
        device = "cuda" if torch.cuda.is_available() else "cpu"
        gpu_memory = None
        if torch.cuda.is_available():
            gpu_memory = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f}GB / {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB"
        
        _model_info = {
            "model_name": model_name,
            "model_type": "NeMo Language Model",
            "parameters": "Unknown",  # Would need model inspection
            "gpu_memory_used": gpu_memory,
            "cuda_available": torch.cuda.is_available(),
            "device": device
        }
        
        logger.info(f"Model loaded successfully: {model_name}")
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        logger.info(f"Device: {device}")
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        _loaded_model = None
        _model_info = {
            "model_name": "Error",
            "model_type": "NeMo (Failed to load)",
            "error": str(e),
            "cuda_available": torch.cuda.is_available(),
            "device": "unknown"
        }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": _loaded_model is not None,
        "cuda_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
    }

@app.get("/model/info", response_model=ModelInfo)
async def get_model_info():
    """Get information about the loaded model"""
    return ModelInfo(**_model_info)

@app.post("/chat/generate", response_model=ChatResponse)
async def generate_chat_response(request: ChatRequest):
    """Generate chat response using loaded NeMo model"""
    import time
    start_time = time.time()
    
    if _loaded_model is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    try:
        # Extract the user message
        if not request.messages:
            raise HTTPException(status_code=400, detail="No messages provided")
        
        # Format messages for NeMo (take the latest user message)
        user_message = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="No user message found")
        
        # Generate response using NeMo model
        # This is a simplified approach - you may need to adapt based on your specific NeMo model
        logger.info(f"Generating response for: {user_message[:50]}...")
        
        # For now, create a structured response based on the model
        # TODO: Implement actual NeMo inference call here
        response_text = await generate_nemo_response(user_message, request)
        
        generation_time = time.time() - start_time
        
        return ChatResponse(
            response=response_text,
            model_info=_model_info,
            generation_time=generation_time
        )
        
    except Exception as e:
        logger.error(f"Generation error: {e}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

async def generate_nemo_response(user_message: str, request: ChatRequest) -> str:
    """Generate response using MegatronGPT model"""
    
    if _loaded_model is None:
        return "Sorry, no language model is currently loaded. Please check the container logs."
    
    try:
        # Check if we have a MegatronGPT model or HuggingFace pipeline
        if hasattr(_loaded_model, 'generate'):
            # This is a MegatronGPT model
            logger.info("Using MegatronGPT model for generation...")
            
            # Prepare inputs for MegatronGPT with context format that works better
            prompt = f"Question: {user_message}\nAnswer: The"
            
            # Use MegatronGPT's generate method with proper NeMo parameter classes
            from nemo.collections.nlp.modules.common.transformer.text_generation import LengthParam, SamplingParam
            
            # Create proper parameter objects for coherent generation  
            length_params = LengthParam(
                max_length=len(prompt.split()) + 30,  # Prompt length + reasonable response
                min_length=len(prompt.split()) + 5   # Prompt length + minimum response
            )
            
            sampling_params = SamplingParam(
                use_greedy=True,  # Use greedy decoding for consistency
                temperature=0.01,  # Very low temperature to avoid randomness
                top_k=1,  # Most deterministic
                top_p=1.0,  # Include all probability mass
                repetition_penalty=2.0,  # Strong penalty against repetition
                add_BOS=False,  # Don't add BOS, might be causing issues
                all_probs=False,
                compute_logprob=False,
                end_strings=["Question:", "\n", "<eos>"]  # Stop at natural breaks
            )
            
            response = _loaded_model.generate(
                inputs=[prompt],
                length_params=length_params,
                sampling_params=sampling_params
            )
            
            # Extract the generated text from MegatronGPT response
            if response and len(response) > 0:
                # MegatronGPT returns a dictionary with generated text
                if isinstance(response, dict) and 'sentences' in response:
                    full_response = response['sentences'][0]
                elif isinstance(response, list) and len(response) > 0:
                    full_response = response[0]
                else:
                    full_response = str(response)
                
                # Remove the original prompt to get just the answer
                generated_text = full_response.replace(prompt, "").strip()
                
                # Clean up the response
                if generated_text:
                    # Stop at natural ending points
                    sentences = generated_text.split('.')
                    if len(sentences) > 1 and sentences[0].strip():
                        generated_text = sentences[0].strip() + '.'
                    response = generated_text
                else:
                    response = "I'm not sure how to answer that."
                    
                logger.info(f"MegatronGPT generated: {response[:100]}...")
            else:
                response = "No response generated."
            
        elif hasattr(_loaded_model, '__call__'):
            # This is a HuggingFace pipeline (fallback)
            logger.info("Using HuggingFace pipeline for generation...")
            
            prompt = f"Question: {user_message}\nAnswer:"
            outputs = _loaded_model(
                prompt,
                max_length=min(request.max_length + len(prompt.split()), 200),
                temperature=max(request.temperature, 0.3),
                top_p=request.top_p,
                top_k=request.top_k,
                do_sample=True,
                repetition_penalty=1.3,
                num_return_sequences=1,
                pad_token_id=_loaded_model.tokenizer.eos_token_id,
                eos_token_id=_loaded_model.tokenizer.eos_token_id
            )
            
            full_response = outputs[0]['generated_text']
            response = full_response[len(prompt):].strip()
            
            # Clean up repetitive text and stop at natural ending points
            if response:
                # Stop at first sentence if it's complete
                sentences = response.split('.')
                if len(sentences) > 1 and sentences[0].strip():
                    response = sentences[0].strip() + '.'
                # Remove repetitive patterns
                lines = response.split('\n')
                response = lines[0].strip()  # Take just the first line
            
            if not response or len(response) < 3:
                response = "I'm not sure how to answer that question."
            
        else:
            response = "Model loaded but generation method not recognized."
            
    except Exception as e:
        logger.error(f"Error during generation: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        response = f"Sorry, I encountered an error while generating a response. I'm still learning how to use the MegatronGPT model effectively."
    
    return response

if __name__ == "__main__":
    logger.info("Starting NeMo API Server on port 8889...")
    uvicorn.run(
        "nemo_api_server:app",
        host="0.0.0.0",
        port=8889,
        reload=False,
        log_level="info"
    )